<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tech | Ray12321]]></title>
  <link href="http://guori12321.github.io/blog/categories/tech/atom.xml" rel="self"/>
  <link href="http://guori12321.github.io/"/>
  <updated>2014-01-08T10:16:32+08:00</updated>
  <id>http://guori12321.github.io/</id>
  <author>
    <name><![CDATA[Ray12321]]></name>
    <email><![CDATA[guori12321@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ssh反向连接]]></title>
    <link href="http://guori12321.github.io/blog/2014/01/02/sshfan-xiang-lian-jie/"/>
    <updated>2014-01-02T13:26:00+08:00</updated>
    <id>http://guori12321.github.io/blog/2014/01/02/sshfan-xiang-lian-jie</id>
    <content type="html"><![CDATA[<p>ssh是广大人民喜闻乐见的远程连接方式。有些时候，实验室里的集群机，只有内部的ip，我们没有办法在实验室外远程连接。寒假就来来临，我想在家连实验室的集群机干活(好吧，这只是个美好的愿望，我估计我会玩一寒假&hellip;)，于是，我就开始研究怎么用ssh反向连接。</p>

<!--more-->


<p>要想实现在家也能访问实验室的集群，首先，你需要一台有固定ip的VPS。在实验室，把集群机反向连接到这台VPS（反射的意思是，你以后可以通过VPS来连接和使用集群，而不是用集群机来使用VPS）。</p>

<p><a href="http://www.cnblogs.com/eshizhan/archive/2012/07/16/2592902.html">这篇具体的ssh反向连接的教程</a>已经讲的很清楚了。下面就记载下我遇到的问题吧。</p>

<h2>VPS的权限问题</h2>

<p>首先，在我用的VPS上，我并没有root权限（开最低限度的权限是很有必要的，也是最安全的）。所以说，那篇教程中的很多命令，我没办法像平时一样用root权限安装在VPS上。</p>

<p>首先查看一下VPS中的Linux的发行版本信息<code>lsb_release -a</code>，好，发现这台VPS是Fedora。我研究了一会没发现怎么安装<code>ss</code>命令，但ss本身只是一个查看的命令，对于我们要做的事情没有影响。</p>

<h2>root权限</h2>

<p>在执行最后一步，也就是在外网的机器上<code>ssh local -pXXXX</code>的时候i需要输入内网主机B的密码。这一步<strong>需要外网主机A的root权限</strong>。如果没有的话，会出现<code>Permission Denied</code>的警告。其实密码是正确的。另外，这个密码是一定要输入的，我添加了把A，B的ssh都添加到对方的<code>authorized_key</code>里了，但还是要输入密码。</p>

<h2>查看端口占用情况</h2>

<p>输入命令<code>netstat -ap|grep 12345</code>即可查看12345端口的使用情况。没有输出表示端口未被占用。</p>

<h2>在后台运行autossh</h2>

<p>使用命令<code>nohup autossh -M 1234 -NR 5678:localhost:9101 username@ip &amp;</code>，重点是<code>nohup</code>和结尾处的<code>&amp;</code>，这样输出就会被送到<code>nohup.out</code>而不是终端中了。</p>

<h2>无响应的情况</h2>

<p>我多次遇到在外网主机A连接内网主机B时无响应的情况。也不报错，就是一直等待。这种情况下只能把内网主机B的有关<code>ssh</code>外网主机A的命令全kill掉，再重连接一次。无响应的原因应该是端口被占用，然后ssh命令在等待端口释放吧。也有可能是实验室的网实在太卡太卡了&hellip;</p>

<p>在<code>autossh</code>之前，记得先把ssh的public key加到外网主机中，不然会报错<code>Exit 1</code>。</p>

<p>如果绑定的时候，出现各种奇怪的错误，出现溜达一圈，买瓶酸奶，回来重新绑定一次，可能就会解决了。谁知道我第一次是哪里输错了&hellip;</p>

<p>如果还是连不上，那么，还是祈祷吧&hellip;三台物理机我都绑定了，但因为实验室的网络实在太差，时断时续，三台基本上只有一台能连过来。</p>

<h2>Future Work</h2>

<p>如果内网的主机重启了的话，以上的工作就要重做一次。应该把这些命令放到内网主机的开机启动项中。不过，真要出现内网主机挂掉的情况的话，寒假不可能有人来重启的&hellip;所以说，我就先不做这部分内容了。</p>

<h2>Conclusion</h2>

<p>真不容易啊，折腾我半天&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[About Pregelix]]></title>
    <link href="http://guori12321.github.io/blog/2013/12/31/wo-de-pregelixlu-xian-tu/"/>
    <updated>2013-12-31T11:16:00+08:00</updated>
    <id>http://guori12321.github.io/blog/2013/12/31/wo-de-pregelixlu-xian-tu</id>
    <content type="html"><![CDATA[<p><a href="http://hyracks.org/projects/pregelix/">Pregelix</a> is an open-source implementation of the bulk-synchronous vertex-oriented programming model (<a href="http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html">Pregel API</a>) for large-scale graph analytics, which scales to very large clusters and very large graphs.</p>

<p>This article aims to recode my study and development of Pregelix applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[人人网点赞狂魔的开发]]></title>
    <link href="http://guori12321.github.io/blog/2013/12/28/dian-zan-kuang-mo-de-kai-fa/"/>
    <updated>2013-12-28T10:19:00+08:00</updated>
    <id>http://guori12321.github.io/blog/2013/12/28/dian-zan-kuang-mo-de-kai-fa</id>
    <content type="html"><![CDATA[<p>之前在人人网上看到一张图，说点赞狂魔眼中的赞，已经不赞，而是<code>朕已阅</code>，于是，我就想搞这么个脚本，来实现这个功能~</p>

<!--more-->


<p>先上效果图：</p>

<p>我用的是<a href="http://tampermonkey.net/">Tampermonkey</a>(简称TM，也就是俗称的油猴)这个Chrome插件来运行我的脚本。之前有直接写过Chrome插件，但是上架和推广比较麻烦，Chrome市场要收费，很坑。这么轻量级的功能，写个小脚本，然后用油猴来运行就好~</p>

<h2>开发过程</h2>

<ul>
<li>找TM的Example</li>
</ul>


<p>Tampermonkey的官网上竟然没有提供任何的example，这一点都不科学！<a href="http://hibbard.eu/tampermonkey-tutorial/">这位同学</a>写出了很详尽的example。仿照它，我们就可以实现我们想做的功能了。</p>

<p>要注意，TM的程序头的那些，不是注释，而是有意义的类似配置文件的部分。比如说，脚本的名称，就是在<code>@name</code>这一行命名的，而不会在保存的时候输入名称。</p>

<ul>
<li><p>查找结点
通过在人人网上审查元素，我发现，有关'赞'的控件的id都是以'ILike'开头的，于是，DFS找到匹配这些id的node，再改掉它的innerHTML就好了。实测每秒运行一次这个脚本更新innerHTML，不会太卡。</p></li>
<li><p>上架
在<a href="http://userscripts.org">userscripts.org</a>上上架。<a href="http://userscripts.org/scripts/show/186905">这里</a>是最终上架后的页面。</p></li>
</ul>


<h2>TODO</h2>

<ol>
<li><p>每秒运行一次脚本的话，效率太低。最好的办法应该是为人人网点赞的JS加上一小段更新innerHTML的部分。</p></li>
<li><p>写一个用户友好的前端，然后把这个脚本上架到Chrome应用商店里。这样就可以让更多更小白的用户来使用这个脚本了。</p></li>
</ol>


<p>以上两点现在还懒得搞&hellip;</p>

<h2>源代码</h2>

<p>已经上传至<a href="https://github.com/guori12321/clickZan">我的Github</a>。</p>

<h2>一些Debug的笔记</h2>

<ul>
<li>不友好的Debug</li>
</ul>


<p>即使我一行代码也没写，Chrome加载完TM也会报错。事实上，有些网站本身在Console里就会报错。更严重的是，在Debug我的脚本的时候，TM只给出有错，但并不给出哪一行有错。非常不友好。</p>

<ul>
<li>关于<code>match</code>部分</li>
</ul>


<p>记得在域名的结尾处加上/*，就像下面这样
<code>
// @match      http://www.baidu.com/*
</code>
而像这样是<strong>不行</strong>的
<code>
// @match      http://www.baidu.com*
</code>
我觉得很奇怪，因为在正则表达式中<code>*</code>是<a href="http://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F">表示零个或多个字符</a>。</p>

<ul>
<li>注意你的输入法！</li>
</ul>


<p>小心在JS中输入了汉语的，或者是全角的空格。这种错误是人眼不可见的。我曾经在某配置文件的行尾多打了个空格，导致整个十一假期都在调试，本来都订好去草原上露宿的帐篷了&hellip;</p>

<ul>
<li>DFS</li>
</ul>


<p>刚开始的时候DFS一次全局元素，非常的慢，不过我也懒的改了&hellip;实测document.getElementsByClass()和JQuery中的$(&ldquo;.classname&rdquo;)都不好用，我不知道怎么把单独的一个元素拿出来再修改innerHTML。对于人人网主页，DFS一次的时间开销可以忽略。就这样吧&hellip;</p>

<ul>
<li>className</li>
</ul>


<p>要取出来元素i的class，要用i.className而不是i.class</p>

<ul>
<li>TM的加载问题</li>
</ul>


<p>有的时候在本地更新完脚本，TM那里并不会加载进去，这时候让TM检查下更新，还不行的话只能重启浏览器了</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[About Hadoop]]></title>
    <link href="http://guori12321.github.io/blog/2013/12/23/wo-de-hadooplu-xian-tu/"/>
    <updated>2013-12-23T17:21:00+08:00</updated>
    <id>http://guori12321.github.io/blog/2013/12/23/wo-de-hadooplu-xian-tu</id>
    <content type="html"><![CDATA[<p>在海量数据计算研究中心一年多了，类似"大数据"这种专业词汇也听了不知道多少次了。只可惜自己还是一知半解，只知道拿着实验室各位老板的名号出来骗人&hellip;现在总算有一点时间了，来安心的研究Hadoop和Prege什么的吧~</p>

<!--more-->


<h2>Mac下安装Hadoop</h2>

<p>显然用<code>homebrew</code>安装是最省事的。<a href="http://importantfish.com/how-to-install-hadoop-on-mac-os-x/">这篇博客</a>中有详细的介绍。</p>

<h2>概览</h2>

<p>Hadoop号称是并行划计算的框架。我之前一直以为，我们把程序的源代码直接扔进去，它就会帮我们来并行的执行了。事实上，我们需要把源代码重新改写，然后Hadoop才能去并行化的执行代码。</p>

<p>我们也可以把Hadoop看成一个类库。每个应用Hadoop的程序，总是要包含大量的Hadoop的包。而为了执行Hadoop中的Map和Reduce这两个步骤，我们需要继承一些类和接口，实现它们中的一些方法。此外，我们的输入和输出文件，还需要放在所谓的Hadoop Distributed File System(HDFS)。</p>

<p>做完以上几步，也就能运行简单的基于Hadoop的程序了。当然，进阶一些的话，还有关于诸如cache之类的我们要细致调整的部分。那些内容等到需要的时候再慢慢研究就好。</p>

<h2>官方的教程</h2>

<p>Hadoop的资料非常多。当然，我们最先要看的，肯定是<a href="http://hadoop.apache.org/index.pdf">Hadoop的官方教程了</a>。一共42页。这是一部面向<strong>使用者</strong>的教程。当然，后期开发中，也要参考<a href="http://hadoop.apache.org/docs/r0.18.3/">官方的文档</a>。</p>

<p>看完这个教程，我发现，Hadoop远比我想象中的要复杂：涉及到太多的模块了。但是，别害怕，我们只是想做一些Hadoop上的应用，而不是进行Hadoop的开发，我们只要按着别人给出的例子，修改修改，做出我们想做的功能就好了。</p>

<h2>官方教程的笔记</h2>

<p>以下是我自己的笔记，仅供参考&hellip;我的子标题，与教程中的子标题是对应的。</p>

<h3>Overview</h3>

<ul>
<li><p>Hadoop的输入输出，都在HDFS中。</p></li>
<li><p>每个Hadoop网络(由集群机中的各个结点组成的网络)，都有一个叫作<code>JobTracker</code>的结点，它负责向各个子结点分配任务，监视各个子结点的状态，并重复执行失败的任务。而每个子结点，都有一个<code>TaskTracker</code>，它负责多进程\线程地执行<code>JobTracker</code>分配给它的任务。</p></li>
<li><p>虽然Hadoop本身是用Java写的，但基于它的应用，可以用其它语言，比如C/C++来写。</p></li>
</ul>


<h3>Inputs and Outputs</h3>

<p>正常的输入输出过程是这样：
<code>
(input)&lt;k1, v1&gt; -&gt; **map** -&gt; &lt;k2, V2&gt; -&gt; **combine** -&gt; &lt;k2, v2&gt; -&gt; **reduce** -&gt; &lt;k3, v3&gt;(output)
</code></p>

<p>可以看出，每一步都是键值对(比如&lt;k1, v1>)。就像关系型数据库用table来表示数据之间的关系，Hadoop使用这种键值对。于是，就有了个问题：所有的输入和输出，都能拆成这样的键值对吗？我问过老师，老师说，大家都是想办法拆出来的，很多时候还是在硬拆&hellip;另一方面，老师也说过，大数据的解决方案中，只有Hadoop是相对成熟的。总之，这就是Hadoop的运行规则。也许以后大家会找到更好的解决方案呢~</p>

<p>此外，可以看出，除了map和reduce这两步，中间还多出一个combine。map这一步，是把输入，映射到一个中间层(intermedia)，原先的输入，可能是一个映射到一个，也有可能是多个映射到一个，所以说，map后会比较乱，这时可能需要把映射后的产物合并或排序，这就是所谓的combine。</p>

<p>PS: 我不知道Hadoop的Map能不能一映射多(Prege中应该是可以)，如有知道的同学，请告诉我，谢谢&hellip;</p>

<h3>Example: WordCount V1.0</h3>

<p>官方的教程给了这么个统计单词个数的例子。</p>

<p>InputFile01:
<code>
Hello World Bye World
</code></p>

<p>InputFile02:
<code>
Hello Hadoop Goodbye Hadoop
</code></p>

<p>Output:
<code>
Bye 1
Goodbye 1
Hadoop 2
Hello 2
World 2
</code></p>

<p>以下是这个例子的源代码。一共58行，不算长。我会在代码中加入汉语的注释来进行解说：
```
package org.myorg;</p>

<p>import java.io.IOException;
import java.util.*;</p>

<p>import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.<em>;
import org.apache.hadoop.io.</em>;
import org.apache.hadoop.mapred.<em>;
import org.apache.hadoop.util.</em>;</p>

<p>//可以看出，是import了很多</p>

<p>//像其它Java程序一样，我们写的程序，也就是一个WordCount类，其中会有一个main函数。
public class WordCount {</p>

<pre><code>//关于Mapper，[官方的文档](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/Mapper.html)的解释是: Interface Mapper&lt;K1,V1,K2,V2&gt;，也就是确定一下这四个变量(输入和输出)的类型，类似于C++中的模板
public static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
//关于final，是指只可被赋值一次的变量。final类型的变量，永远指向同一个object，但这个object本身可能会改变。更多的内容，参考[Java的文档](http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.12.4)
  private final static IntWritable one = new IntWritable(1);

  private Text word = new Text();

  public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {
    String line = value.toString();
    StringTokenizer tokenizer = new StringTokenizer(line);
    while (tokenizer.hasMoreTokens()) {
      word.set(tokenizer.nextToken());
      output.collect(word, one);
    }
  }
}

public static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
  public void reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {
    int sum = 0;
    while (values.hasNext()) {
      sum += values.next().get();
    }
    output.collect(key, new IntWritable(sum));
  }
}

public static void main(String[] args) throws Exception {
  JobConf conf = new JobConf(WordCount.class);
  conf.setJobName("wordcount");

  conf.setOutputKeyClass(Text.class);
  conf.setOutputValueClass(IntWritable.class);

  conf.setMapperClass(Map.class);
  conf.setCombinerClass(Reduce.class);
  conf.setReducerClass(Reduce.class);

  conf.setInputFormat(TextInputFormat.class);
  conf.setOutputFormat(TextOutputFormat.class);

  FileInputFormat.setInputPaths(conf, new Path(args[0]));
  FileOutputFormat.setOutputPath(conf, new Path(args[1]));

  JobClient.runJob(conf);
}
</code></pre>

<p>}
```</p>

<p>翻译一下官方的解释吧，这几十行实在看不大懂：</p>

<p>这几行代码真是简洁明快(你妹！)，Mapper类中，通过map方法，每次处理一行(这些输入的行的类型，在main方法中定义了(TextInputFormat))。之后，它把每行都通过空格来分割(通过StringTokenizer)，然后生成一个类似于<code>&lt; &lt;word&gt;, 1&gt;</code>的键值对。</p>

<p>第一个输入文件在map后会生成</p>

<p><code>
&lt; Hello, 1&gt;
&lt; World, 1&gt;
&lt; Bye, 1&gt;
&lt; World, 1&gt;
</code></p>

<p>第二个文件</p>

<p><code>
&lt; Hello, 1&gt;
&lt; Hadoop, 1&gt;
&lt; Goodbye, 1&gt;
&lt; Hadoop, 1&gt;
</code></p>

<p>再之后，是<strong>排序</strong>并把上述的中间结果进行<strong>combine</strong> (main方法中定义了combine的方法)。注意，这里是每个文件的结果各自combine。这里的combine，也可以看作是map。</p>

<p>第一个文件会变成</p>

<p><code>
&lt; Bye, 1&gt;
&lt; Hello, 1&gt;
&lt; World, 2&gt;
</code></p>

<p>第二个文件</p>

<p><code>
&lt; Goodbye, 1&gt;
&lt; Hadoop, 2&gt;
&lt; Hello, 1&gt;
</code></p>

<p>再之后就是reduce了。它对应的代码与map类似。这时候，两个文件产生的中间结果，就会被排序并合并/缩减到最终的结果：</p>

<p><code>
&lt; Bye, 1&gt;
&lt; Goodbye, 1&gt;
&lt; Hadoop, 2&gt;
&lt; Hello, 2&gt;
&lt; World, 2&gt;
</code></p>

<h2>教材中的其它部分</h2>

<p>在上面的例子之后，教材中详细讲了怎样运行这个例子，这一部分需要一些命令行的知识，传的参数也是非常的多。再之后，则是讲Mapper和Reducer等具体和哪些Hadoop中的类相关。这些内容就非常具体了，大家慢慢看就好。</p>

<p>写到这里，不知道算不算有用呢？有需要的时候我再回来补充吧。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Yogurt OS Development Week0]]></title>
    <link href="http://guori12321.github.io/blog/2013/12/22/yogurt-os-development-week0/"/>
    <updated>2013-12-22T12:25:00+08:00</updated>
    <id>http://guori12321.github.io/blog/2013/12/22/yogurt-os-development-week0</id>
    <content type="html"><![CDATA[<p>After the busy graduate school application period, nowadays I have plenty of time to do what I want. Developing a operating system sounds cool and awesome, and as I always wonder how it is developed, I decide to develop my own one: YogurtOS!</p>

<p>Why Yogurt? Because I love it!</p>

<!--more-->


<p>There is a famous OS development tutorial <a href="http://book.douban.com/subject/11530329/">Develop your own OS in 30 days</a>. The book is written in Japanese and I read the Chinese translation version.</p>

<p>The following is my experience in developing such a OS.</p>

<h2>Day 0 2013-12-21</h2>

<p>I read the Day 0 part of the book.</p>
]]></content>
  </entry>
  
</feed>
